#!/bin/bash
# Script per aggiornare le GitHub issues con dettagli completi
# Esegui con: ./scripts/update_github_issues.sh
# Prerequisiti: gh auth login

set -e

echo "ğŸ”„ Aggiornamento GitHub Issues con specifiche dettagliate..."
echo ""

# US-002: Setup Qdrant Vector Store
echo "ğŸ“ Aggiornando US-002..."
gh issue edit 7 --body "**Come** data scientist
**Voglio** un'istanza Qdrant configurata
**Per** poter indicizzare e cercare i CV

## ğŸ¯ Acceptance Criteria
- [ ] Qdrant running via docker-compose
- [ ] Collection \`cv_skills\` creata con schema corretto
- [ ] Collection \`cv_experiences\` creata con schema corretto
- [ ] Script di inizializzazione collections
- [ ] Script di test connessione
- [ ] Health check endpoint \`/health\` include Qdrant status

## ğŸ”§ Technical Stack
- **Vector Store:** Qdrant (latest)
- **Client:** \`qdrant-client\` Python SDK
- **Container:** Docker Compose

## ğŸ“ File da creare
\`\`\`
â”œâ”€â”€ docker-compose.yml (add Qdrant service)
â”œâ”€â”€ src/services/qdrant/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py          # Singleton client
â”‚   â”œâ”€â”€ collections.py     # Schema definitions
â”‚   â””â”€â”€ health.py          # Health check
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_qdrant.py     # Initialize collections
â””â”€â”€ tests/
    â””â”€â”€ test_qdrant_connection.py
\`\`\`

## ğŸ“ Collection Schema - cv_skills
\`\`\`python
{
    \"vectors\": {\"size\": 1536, \"distance\": \"Cosine\"},
    \"payload_schema\": {
        \"cv_id\": \"keyword\",
        \"normalized_skills\": \"keyword[]\",
        \"skill_domain\": \"keyword\",
        \"seniority_bucket\": \"keyword\",
        \"dictionary_version\": \"keyword\"
    }
}
\`\`\`

## âœ… Definition of Done
- [ ] \`make docker-up\` avvia Qdrant
- [ ] Collections create con schema corretto
- [ ] \`/health\` endpoint include Qdrant status
- [ ] Test connessione passa in CI
- [ ] README aggiornato con setup Qdrant

## ğŸŒ¿ Feature Branch
\`feature/US-002-qdrant-setup\`

**Story Points:** 5"

# US-003: Parser CV DOCX
echo "ğŸ“ Aggiornando US-003..."
gh issue edit 8 --body "**Come** sistema
**Voglio** estrarre testo strutturato dai CV in formato DOCX
**Per** poter processare i curriculum aziendali

## ğŸ¯ Acceptance Criteria
- [ ] Parsing sezioni: Skills, Esperienze, Formazione, Certificazioni
- [ ] Estrazione metadata: nome, cognome, ruolo attuale
- [ ] Gestione errori per file malformati o corrotti
- [ ] Supporto encoding UTF-8 e caratteri speciali
- [ ] Unit test con almeno 5 CV di esempio
- [ ] Performance: < 2 sec per CV

## ğŸ”§ Technical Stack
- **DOCX Parsing:** \`python-docx\`
- **Text Processing:** regex
- **Validation:** pydantic

## ğŸ“ File da creare
\`\`\`
â”œâ”€â”€ src/core/parser/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ docx_parser.py     # Main parser
â”‚   â”œâ”€â”€ section_detector.py
â”‚   â”œâ”€â”€ metadata_extractor.py
â”‚   â””â”€â”€ schemas.py         # Pydantic models
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ fixtures/
â”‚   â”‚   â””â”€â”€ sample_cvs/    # 5+ sample CVs
â”‚   â””â”€â”€ test_cv_parser.py
â””â”€â”€ docs/
    â””â”€â”€ cv_format_guide.md
\`\`\`

## ğŸ“ Output Schema
\`\`\`python
class ParsedCV(BaseModel):
    metadata: CVMetadata
    skills: SkillSection
    experiences: list[ExperienceItem]
    education: list[str]
    certifications: list[str]
    raw_text: str
\`\`\`

## âš ï¸ Edge Cases da Gestire
- CV senza sezioni chiare
- Tabelle con skill
- File protetti da password
- Encoding non-UTF8

## âœ… Definition of Done
- [ ] Parser estrae tutte le sezioni
- [ ] Almeno 5 CV di test diversi
- [ ] Coverage test â‰¥ 80%
- [ ] Gestione errori documentata
- [ ] Performance validata (< 2 sec/CV)

## ğŸŒ¿ Feature Branch
\`feature/US-003-cv-parser\`

**Story Points:** 8"

# US-004: Skill Extraction e Normalizzazione
echo "ğŸ“ Aggiornando US-004..."
gh issue edit 9 --body "**Come** data scientist
**Voglio** estrarre e normalizzare le skill dai CV
**Per** avere un vocabolario controllato di competenze

## ğŸ¯ Acceptance Criteria
- [ ] Dizionario skill base con 100+ entry
- [ ] Mapping sinonimi â†’ skill normalizzate
- [ ] Confidence score per ogni mapping (0.0-1.0)
- [ ] Log skill non riconosciute per review
- [ ] Categorizzazione skill per domain
- [ ] Versioning dizionario

## ğŸ”§ Technical Stack
- **Matching:** \`rapidfuzz\` (fuzzy matching), regex
- **Storage:** YAML per dizionario

## ğŸ“ File da creare
\`\`\`
â”œâ”€â”€ src/core/skills/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extractor.py       # Main extraction logic
â”‚   â”œâ”€â”€ normalizer.py      # Normalization engine
â”‚   â”œâ”€â”€ dictionary.py      # Dictionary loader
â”‚   â””â”€â”€ schemas.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ skills_dictionary.yaml
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_skill_extraction.py
â””â”€â”€ scripts/
    â””â”€â”€ analyze_unknown_skills.py
\`\`\`

## ğŸ“ Dictionary Schema
\`\`\`yaml
version: \"1.0.0\"
skills:
  python:
    canonical: \"python\"
    domain: \"backend\"
    aliases: [\"py\", \"python3\"]
\`\`\`

## ğŸ”„ Matching Strategy
1. Exact match (confidence: 1.0)
2. Alias match (confidence: 0.95)
3. Fuzzy match threshold 0.85
4. Unknown â†’ log for review

## âœ… Definition of Done
- [ ] Dizionario con 100+ skill
- [ ] Mapping testato su 50+ skill reali
- [ ] Confidence score coerente
- [ ] Script report unknown skills
- [ ] Unit test per edge cases

## ğŸŒ¿ Feature Branch
\`feature/US-004-skill-extraction\`

**Story Points:** 13"

# US-005: Embedding e Indexing Pipeline
echo "ğŸ“ Aggiornando US-005..."
gh issue edit 10 --body "**Come** sistema
**Voglio** generare embedding e indicizzare in Qdrant
**Per** abilitare la ricerca semantica

## ğŸ¯ Acceptance Criteria
- [ ] Embedding con OpenAI text-embedding-ada-002 (o alternative)
- [ ] Upsert in collection \`cv_skills\` con payload completo
- [ ] Upsert in collection \`cv_experiences\` con payload completo
- [ ] Metadata completi su ogni punto
- [ ] Pipeline idempotente (re-run safe)
- [ ] Batch processing per performance

## ğŸ”§ Technical Stack
- **Embedding:** OpenAI API / \`sentence-transformers\`
- **Vector Store:** Qdrant
- **Queue (optional):** Redis

## ğŸ“ File da creare
\`\`\`
â”œâ”€â”€ src/services/embedding/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ openai_client.py
â”‚   â””â”€â”€ local_embedder.py
â”œâ”€â”€ src/core/indexing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ cv_indexer.py
â”‚   â””â”€â”€ batch_processor.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ index_single_cv.py
â”‚   â””â”€â”€ index_all_cvs.py
â””â”€â”€ tests/
    â””â”€â”€ test_indexing_pipeline.py
\`\`\`

## ğŸ“ Pipeline Flow
\`\`\`
ParsedCV
    â”œâ”€â”€â–º Skill Section â”€â”€â–º Embedding â”€â”€â–º cv_skills
    â””â”€â”€â–º Experience Items â”€â”€â–º Embedding â”€â”€â–º cv_experiences
\`\`\`

## ğŸ”„ Idempotency Strategy
- Deterministic point ID: \`{cv_id}_{section}\`
- Upsert updates existing, inserts new

## âœ… Definition of Done
- [ ] Pipeline end-to-end funzionante
- [ ] Metadata completi su ogni punto
- [ ] Re-run non duplica dati
- [ ] Performance: < 5 sec per CV
- [ ] Test con 10+ CV reali

## ğŸŒ¿ Feature Branch
\`feature/US-005-embedding-pipeline\`

**Story Points:** 13"

# US-006: API Ricerca Profili per Skill
echo "ğŸ“ Aggiornando US-006..."
gh issue edit 11 --body "**Come** utente
**Voglio** cercare profili in base a skill richieste
**Per** trovare candidati con competenze specifiche

## ğŸ¯ Acceptance Criteria
- [ ] Endpoint \`POST /api/v1/search/skills\`
- [ ] Input: lista skill, filtri opzionali (seniority, domain)
- [ ] Output: lista profili ranked con score
- [ ] Paginazione risultati (limit, offset)
- [ ] Response time < 500ms
- [ ] OpenAPI documentation

## ğŸ”§ Technical Stack
- **API:** FastAPI
- **Validation:** Pydantic
- **Docs:** OpenAPI 3.0

## ğŸ“ File da creare
\`\`\`
â”œâ”€â”€ src/api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ v1/
â”‚       â”œâ”€â”€ router.py
â”‚       â”œâ”€â”€ search.py
â”‚       â””â”€â”€ schemas.py
â”œâ”€â”€ src/services/search/
â”‚   â”œâ”€â”€ skill_search.py
â”‚   â””â”€â”€ scoring.py
â””â”€â”€ tests/api/
    â””â”€â”€ test_search_endpoints.py
\`\`\`

## ğŸ“ Request/Response Schema
\`\`\`python
# Request
class SkillSearchRequest(BaseModel):
    skills: list[str]
    filters: Optional[SearchFilters]
    limit: int = 10
    offset: int = 0

# Response
class ProfileMatch(BaseModel):
    cv_id: str
    score: float
    matched_skills: list[str]
    missing_skills: list[str]
\`\`\`

## âœ… Definition of Done
- [ ] Endpoint funzionante e documentato
- [ ] Response < 500ms (testato)
- [ ] Paginazione corretta
- [ ] Test coverage â‰¥ 80%
- [ ] OpenAPI spec validata

## ğŸŒ¿ Feature Branch
\`feature/US-006-search-api\`

**Story Points:** 8"

# US-007: Filtro DisponibilitÃ 
echo "ğŸ“ Aggiornando US-007..."
gh issue edit 12 --body "**Come** utente
**Voglio** filtrare i profili per stato di disponibilitÃ 
**Per** vedere solo candidati effettivamente assegnabili

## ğŸ¯ Acceptance Criteria
- [ ] Filtri: \`only_free\`, \`free_or_partial\`, \`any\`
- [ ] Integrazione con source stato operativo (SharePoint/Excel)
- [ ] Cache stato con TTL configurabile
- [ ] Risposta esplicita se nessuno disponibile
- [ ] Aggiornamento stato asincrono

## ğŸ”§ Technical Stack
- **Cache:** Redis
- **Data Source:** SharePoint List / Excel (inizialmente)
- **Scheduler:** APScheduler

## ğŸ“ File da creare
\`\`\`
â”œâ”€â”€ src/services/availability/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ service.py
â”‚   â”œâ”€â”€ cache.py
â”‚   â”œâ”€â”€ source_sharepoint.py
â”‚   â””â”€â”€ source_excel.py
â”œâ”€â”€ src/core/filters/
â”‚   â””â”€â”€ availability_filter.py
â””â”€â”€ tests/
    â””â”€â”€ test_availability_service.py
\`\`\`

## ğŸ“ Availability States
\`\`\`python
class AvailabilityStatus(Enum):
    FREE = \"free\"           # Completamente disponibile
    PARTIAL = \"partial\"     # Allocato parzialmente
    BUSY = \"busy\"           # Allocato su progetto
    UNAVAILABLE = \"unavailable\"
\`\`\`

## ğŸ”„ Cache Strategy
- Redis key: \`availability:{cv_id}\`
- TTL: 1 hour (configurable)

## âœ… Definition of Done
- [ ] Filtri funzionanti su tutti i modi
- [ ] Cache Redis operativa
- [ ] Refresh automatico configurato
- [ ] Messaggio esplicito se 0 risultati
- [ ] Test con dati mock

## ğŸŒ¿ Feature Branch
\`feature/US-007-availability-filter\`

**Story Points:** 5"

# US-008: Match con Job Description
echo "ğŸ“ Aggiornando US-008..."
gh issue edit 13 --body "**Come** utente
**Voglio** trovare il miglior profilo per una job description
**Per** proporre candidati ad opportunitÃ  specifiche

## ğŸ¯ Acceptance Criteria
- [ ] Endpoint \`POST /api/v1/match/job\`
- [ ] Input: testo job description (free text)
- [ ] Estrazione automatica skill richieste dalla JD
- [ ] Ranking profili con spiegazione LLM
- [ ] Output strutturato con motivazione per ogni match
- [ ] Distinzione tra must-have e nice-to-have skills

## ğŸ”§ Technical Stack
- **LLM:** OpenAI GPT-4 / Azure OpenAI
- **Extraction:** LLM-based skill extraction
- **Ranking:** Vector similarity + LLM reasoning

## ğŸ“ File da creare
\`\`\`
â”œâ”€â”€ src/api/v1/
â”‚   â””â”€â”€ job_match.py
â”œâ”€â”€ src/services/matching/
â”‚   â”œâ”€â”€ job_analyzer.py
â”‚   â”œâ”€â”€ candidate_ranker.py
â”‚   â””â”€â”€ explainer.py
â”œâ”€â”€ src/core/llm/
â”‚   â”œâ”€â”€ client.py
â”‚   â””â”€â”€ prompts.py
â””â”€â”€ tests/
    â””â”€â”€ test_job_matching.py
\`\`\`

## ğŸ“ Flow
\`\`\`
Job Description
      â”‚
      â–¼
LLM Extraction â”€â”€â–º Required Skills
      â”‚
      â–¼
Vector Search â”€â”€â–º Candidate Shortlist (K=20)
      â”‚
      â–¼
Availability Filter â”€â”€â–º Filtered Candidates
      â”‚
      â–¼
LLM Ranking â”€â”€â–º Top N with Explanations
\`\`\`

## ğŸ¤– LLM Parameters
- Model: GPT-4 / GPT-4-turbo
- Temperature: 0.1 (deterministic)
- Max tokens: 2000
- Response format: JSON mode

## âœ… Definition of Done
- [ ] Endpoint funzionante end-to-end
- [ ] Estrazione skill accurata (test su 5 JD)
- [ ] Spiegazioni coerenti e utili
- [ ] Response time < 10 sec
- [ ] Test con JD reali

## ğŸŒ¿ Feature Branch
\`feature/US-008-job-match\`

**Story Points:** 13"

echo ""
echo "âœ… Tutte le issues aggiornate!"
echo ""
echo "ğŸ“‹ Riepilogo:"
echo "  - US-002: Setup Qdrant Vector Store"
echo "  - US-003: Parser CV DOCX"
echo "  - US-004: Skill Extraction e Normalizzazione"
echo "  - US-005: Embedding e Indexing Pipeline"
echo "  - US-006: API Ricerca Profili per Skill"
echo "  - US-007: Filtro DisponibilitÃ "
echo "  - US-008: Match con Job Description"
echo ""
echo "âš ï¸  Nota: I numeri delle issue (#7-#13) potrebbero variare."
echo "   Verifica i numeri corretti con: gh issue list"
